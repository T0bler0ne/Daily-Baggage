{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/T0bler0ne/Daily-Baggage/blob/main/scripts/convert_model_to_long.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aad_1s7ybD5o"
      },
      "source": [
        "# `RoBERTa` --> `Longformer`: build a \"long\" version of pretrained models\n",
        "\n",
        "This notebook replicates the procedure descriped in the [Longformer paper](https://arxiv.org/abs/2004.05150) to train a Longformer model starting from the RoBERTa checkpoint. The same procedure can be applied to build the \"long\" version of other pretrained models as well.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BieXv0YUd7NF"
      },
      "source": [
        "### Data, libraries, and imports\n",
        "Our procedure requires a corpus for pretraining. For demonstration, we will use Wikitext103; a corpus of 100M tokens from wikipedia articles. Depending on your application, consider using a different corpus that is a better match."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AZZ4VmKSwvH"
      },
      "outputs": [],
      "source": [
        "!mkdir -p data/Repos data/Repos-cleaned\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x /content/data/cloner.sh\n",
        "!bash /content/data/cloner.sh\n"
      ],
      "metadata": {
        "id": "-FXhZvmx0jw9",
        "outputId": "59623e0b-e580-46f5-c58b-9170b2ab8757",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0xProject/0x.js e25cc301fddbc67f793ca0eb0f7635cdb9147a71\n",
            "0xProject/contracts d80460d94daf8725b0017ff40c81f02a9a8f7f89\n",
            "1backend/1backend 29869b6b160feb764b5a4f9f1984a9d1db0bed80\n",
            "2fd/graphdoc a5bbc7b601975b00ec83b781a6afe6014ebe171b\n",
            "43081j/rar.js b5c577235d905382082429cff4f8666106090a90\n",
            "500tech/angular-tree-component 3d6c603ce28ee33174f0db40c69bcdfd4b9bddfa\n",
            "5calls/5calls 70deaf479d60883cfc9b0d2f49a7297d812759bb\n",
            "74th/vscode-vim 1db62fd74b5dce48e12a732e001472ff6bdec5a4\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n",
            "fatal: cannot change to 'Repos/0xProject/contracts': No such file or directory\n",
            "aberezkin/ng2-image-upload 89a891b4a7d7d34a91434308488c027149f4fa1b\n",
            "HEAD is now at b5c5772 Merge pull request #8 from 43081j/typescript\n",
            "accounts-js/accounts b90ef6e32011ceca44b4c15df76a333aa7ad0e52\n",
            "HEAD is now at 29869b6 Fixing delete project (#148)\n",
            "acekyd/made-in-nigeria cace557f9437a60d309a71e9a88db135b8a349db\n",
            "HEAD is now at 1db62fd fix #64 #69\n",
            "ademilter/bricklayer 45c868bcef8894c7c013316e95385014c6117c3f\n",
            "HEAD is now at 89a891b fix: url changed by onBeforeUpload is ignored (#182)\n",
            "adriancarriger/angularfire2-offline a14b36bd29e77bea712d53c8d8175772b38efabd\n",
            "HEAD is now at 45c868b Merge pull request #36 from rdunk/master\n",
            "AFASSoftware/maquette 0665d00bd7ca2f2ca0bd9756c1535017f5fa2ab7\n",
            "HEAD is now at a5bbc7b Added \".graphqls\" and \".gqls\" as valid schema file endings (#39)\n",
            "afrad/angular2-websocket 1da132c3b8108c2e03ed98d555ad7c96e57c1878\n",
            "HEAD is now at 70deaf4 Better handling of split districts by declaring the contacts we intenâ€¦ (#390)\n",
            "aggarwalankush/ionic-mosum cb26d4bfa3706debd848e6a94a34122f9fa780e8\n",
            "HEAD is now at 1da132c Bump 9.5.6\n",
            "aggarwalankush/ionic-push-base 22d3695576450bbe41d8325b6d9e72cde9f21c72\n",
            "HEAD is now at 0665d00 3.2.2\n",
            "ag-grid/ag-grid 8f5196f24b303a39cd56bb5ff879f24dd448f16a\n",
            "HEAD is now at cb26d4b updated icons\n",
            "ag-grid/ag-grid-angular 5948e02c4ce9c0317ac219b9e3b0c234a5d0830f\n",
            "HEAD is now at 22d3695 Console.log doesn't support multiple arguments on Android\n",
            "ahomu/Talkie 694f88ef26fd0f55b6a8833e7d376cbc46111189\n",
            "HEAD is now at cace557 Merge pull request #93 from ichtrojan/patch-1\n",
            "aikoven/typescript-fsa 860c8fca8c5ac4ff15586db395ea1ca27be3eb76\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n",
            "fatal: cannot change to 'Repos/ag-grid/ag-grid-angular': No such file or directory\n",
            "aioutecism/amVim-for-VSCode 0e610b5a9d3931e647969ba92aba9fe8b6bf18eb\n",
            "HEAD is now at 860c8fc Make greenkeeper ignore rollup updates\n",
            "airbrake/airbrake-js c4516f3848099b9d1b8102a5584e6b38c5b0d4c2\n",
            "HEAD is now at a14b36b docs(readme): update version support\n",
            "ajtoo/vscode-org-mode 6d3fbbb97657076c8be9f8d6075a8697c250df01\n",
            "HEAD is now at 694f88e Merge pull request #38 from ahomu/greenkeeper/postcss-cli-4.1.0\n",
            "ajtowf/ng2_play 779809fc85ccded92d9fce084a91b24ec9b43618\n",
            "HEAD is now at 0e610b5 Bump version.\n",
            "akfish/node-vibrant 1b84e57b88f9e6128ca1d5d3e44409e8dbd02a80\n",
            "HEAD is now at b90ef6e3 Update dependency rimraf to v2.6.2\n",
            "akserg/ng2-dnd 81306c1f30c3fae7c45a5e3b7cd7006efc5e18b2\n",
            "HEAD is now at 779809f Fixed all lint errors.\n",
            "akserg/ng2-slim-loading-bar bb91b83fe8711c11efd6bd3b9c8d395c62eeee0b\n",
            "HEAD is now at bb91b83 build($compile): Fix for Travis Ci build\n",
            "akserg/ng2-toasty 631ac12e7facac4d438837b65e2916db30bbd6e2\n",
            "HEAD is now at 81306c1 Fixed stye in README\n",
            "akveo/nebular f4fceb5d513b75b2ccbbf8ab7097d28ff3fdb435\n",
            "HEAD is now at 1b84e57 Merge pull request #59 from lasseborly/patch-1\n",
            "akveo/ng2-smart-table 849a3d5daaaad94893a332756279f0f00bff289f\n",
            "HEAD is now at 631ac12 fix($compile): Fix for error: unexpected Token: /*\n",
            "akveo/ngx-admin e7bca5c1b79fde73c7fac4fee7b2bbed365d52f5\n",
            "HEAD is now at c4516f3 v1.0.6\n",
            "alamgird/angular-next-starter-kit 9dc8608d0a81f417a1fd25dd098a954ac1bdd0f2\n",
            "HEAD is now at 3d6c603 Update README.md\n",
            "Alberplz/angular2-color-picker 1c29ca1c49afbe832826899a9c04d1c061d170e3\n",
            "HEAD is now at 9dc8608 Merge pull request #11 from rinogo/master\n",
            "alefragnani/vscode-project-manager 71f76ba24c2c7a04db5fa809499d22a074bb116d\n",
            "HEAD is now at 1c29ca1 Version 1.3.1\n",
            "alex3165/react-mapbox-gl f1f5b97c5f4c400fe2e4e89ccaab9db02b8e8229\n",
            "HEAD is now at 71f76ba clean up readme (new in version)\n",
            "alexjlockwood/avocado bba21b828073bac745649b9c46f6bdb7b15e5cc8\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install build-essential\n",
        "!pip install --upgrade pip setuptools wheel\n"
      ],
      "metadata": {
        "id": "8O86CfzwkV4e",
        "outputId": "2cb6e89c-602a-4944-ca72-86af7241db73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.3.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (75.6.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.45.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y build-essential\n"
      ],
      "metadata": {
        "id": "RkkE-VELsEXj",
        "outputId": "6b054447-1eef-4cf7-9e56-707329f45c38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers==0.11.6\n"
      ],
      "metadata": {
        "id": "R7DpXL2Pk4FT",
        "outputId": "454ed148-fc74-43e6-f5e0-884995beb49e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers==0.11.6 in /usr/local/lib/python3.10/dist-packages (0.11.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.16.0\n"
      ],
      "metadata": {
        "id": "5QXTME5YpBrk",
        "outputId": "6a0de809-b586-4025-d17f-5cc95514363a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.16.0 in /usr/local/lib/python3.10/dist-packages (4.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (2.32.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (0.1.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (0.11.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.0) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.0) (2024.12.14)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.16.0) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.16.0) (1.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "U0NnMMl6wy7Q"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import math\n",
        "import copy\n",
        "import torch\n",
        "from dataclasses import dataclass, field\n",
        "from transformers import RobertaForMaskedLM, RobertaTokenizerFast, TextDataset, DataCollatorForLanguageModeling, Trainer\n",
        "from transformers import TrainingArguments, HfArgumentParser\n",
        "from transformers.models.longformer import LongformerSelfAttention\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgoNVJYUbD59"
      },
      "source": [
        "### RobertaLong\n",
        "\n",
        "`RobertaLongForMaskedLM` represents the \"long\" version of the `RoBERTa` model. It replaces `BertSelfAttention` with `RobertaLongSelfAttention`, which is a thin wrapper around `LongformerSelfAttention`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "J9EBISkRxPjO"
      },
      "outputs": [],
      "source": [
        "class RobertaLongSelfAttention(LongformerSelfAttention):\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        return super().forward(hidden_states, attention_mask=attention_mask, output_attentions=output_attentions)\n",
        "\n",
        "\n",
        "class RobertaLongForMaskedLM(RobertaForMaskedLM):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        for i, layer in enumerate(self.roberta.encoder.layer):\n",
        "            # replace the `modeling_bert.BertSelfAttention` object with `LongformerSelfAttention`\n",
        "            layer.attention.self = RobertaLongSelfAttention(config, layer_id=i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LRZa5s1bD6E"
      },
      "source": [
        "Starting from the `roberta-base` checkpoint, the following function converts it into an instance of `RobertaLong`. It makes the following changes:\n",
        "\n",
        "- extend the position embeddings from `512` positions to `max_pos`. In Longformer, we set `max_pos=4096`\n",
        "\n",
        "- initialize the additional position embeddings by copying the embeddings of the first `512` positions. This initialization is crucial for the model performance (check table 6 in [the paper](https://arxiv.org/pdf/2004.05150.pdf) for performance without this initialization)\n",
        "\n",
        "- replaces `modeling_bert.BertSelfAttention` objects with `modeling_longformer.LongformerSelfAttention` with a attention window size `attention_window`\n",
        "\n",
        "The output of this function works for long documents even without pretraining. Check tables 6 and 11 in [the paper](https://arxiv.org/pdf/2004.05150.pdf) to get a sense of the expected performance of this model before pretraining."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-m4A_ttixPuf"
      },
      "outputs": [],
      "source": [
        "def create_long_model(save_model_to, attention_window, max_pos):\n",
        "    model = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', model_max_length=max_pos)\n",
        "    config = model.config\n",
        "\n",
        "    # extend position embeddings\n",
        "    tokenizer.model_max_length = max_pos\n",
        "    tokenizer.init_kwargs['model_max_length'] = max_pos\n",
        "    current_max_pos, embed_size = model.roberta.embeddings.position_embeddings.weight.shape\n",
        "    max_pos += 2  # NOTE: RoBERTa has positions 0,1 reserved, so embedding size is max position + 2\n",
        "    config.max_position_embeddings = max_pos\n",
        "    assert max_pos > current_max_pos\n",
        "    # allocate a larger position embedding matrix\n",
        "    new_pos_embed = model.roberta.embeddings.position_embeddings.weight.new_empty(max_pos, embed_size)\n",
        "    # copy position embeddings over and over to initialize the new position embeddings\n",
        "    k = 2\n",
        "    step = current_max_pos - 2\n",
        "    while k < max_pos - 1:\n",
        "        new_pos_embed[k:(k + step)] = model.roberta.embeddings.position_embeddings.weight[2:]\n",
        "        k += step\n",
        "    model.roberta.embeddings.position_embeddings.weight.data = new_pos_embed\n",
        "    model.roberta.embeddings.position_ids.data = torch.tensor([i for i in range(max_pos)]).reshape(1, max_pos)\n",
        "\n",
        "    # replace the `modeling_bert.BertSelfAttention` object with `LongformerSelfAttention`\n",
        "    config.attention_window = [attention_window] * config.num_hidden_layers\n",
        "    for i, layer in enumerate(model.roberta.encoder.layer):\n",
        "        longformer_self_attn = LongformerSelfAttention(config, layer_id=i)\n",
        "        longformer_self_attn.query = layer.attention.self.query\n",
        "        longformer_self_attn.key = layer.attention.self.key\n",
        "        longformer_self_attn.value = layer.attention.self.value\n",
        "\n",
        "        longformer_self_attn.query_global = copy.deepcopy(layer.attention.self.query)\n",
        "        longformer_self_attn.key_global = copy.deepcopy(layer.attention.self.key)\n",
        "        longformer_self_attn.value_global = copy.deepcopy(layer.attention.self.value)\n",
        "\n",
        "        layer.attention.self = longformer_self_attn\n",
        "\n",
        "    logger.info(f'saving model to {save_model_to}')\n",
        "    model.save_pretrained(save_model_to)\n",
        "    tokenizer.save_pretrained(save_model_to)\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkbQvhOMbD6L"
      },
      "source": [
        "Pretraining on Masked Language Modeling (MLM) doesn't update the global projection layers. After pretraining, the following function copies `query`, `key`, `value` to their global counterpart projection matrices.\n",
        "For more explanation on \"local\" vs. \"global\" attention, please refer to the documentation [here](https://huggingface.co/transformers/model_doc/longformer.html#longformer-self-attention)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CO3MoEgCxP9W"
      },
      "outputs": [],
      "source": [
        "def copy_proj_layers(model):\n",
        "    for i, layer in enumerate(model.roberta.encoder.layer):\n",
        "        layer.attention.self.query_global = copy.deepcopy(layer.attention.self.query)\n",
        "        layer.attention.self.key_global = copy.deepcopy(layer.attention.self.key)\n",
        "        layer.attention.self.value_global = copy.deepcopy(layer.attention.self.value)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KjI9f8BbD6S"
      },
      "source": [
        "### Pretrain and Evaluate on masked language modeling (MLM)\n",
        "\n",
        "The following function pretrains and evaluates a model on MLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wC2sddeLyVhA"
      },
      "outputs": [],
      "source": [
        "def pretrain_and_evaluate(args, model, tokenizer, eval_only, model_path):\n",
        "    val_dataset = TextDataset(tokenizer=tokenizer,\n",
        "                              file_path=args.val_datapath,\n",
        "                              block_size=tokenizer.model_max_length)\n",
        "    if eval_only:\n",
        "        train_dataset = val_dataset\n",
        "    else:\n",
        "        logger.info(f'Loading and tokenizing training data is usually slow: {args.train_datapath}')\n",
        "        train_dataset = TextDataset(tokenizer=tokenizer,\n",
        "                                    file_path=args.train_datapath,\n",
        "                                    block_size=tokenizer.model_max_length)\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "    trainer = Trainer(model=model, args=args, data_collator=data_collator,\n",
        "                      train_dataset=train_dataset, eval_dataset=val_dataset, prediction_loss_only=True,)\n",
        "\n",
        "    eval_loss = trainer.evaluate()\n",
        "    eval_loss = eval_loss['eval_loss']\n",
        "    logger.info(f'Initial eval bpc: {eval_loss/math.log(2)}')\n",
        "\n",
        "    if not eval_only:\n",
        "        trainer.train(model_path=model_path)\n",
        "        trainer.save_model()\n",
        "\n",
        "        eval_loss = trainer.evaluate()\n",
        "        eval_loss = eval_loss['eval_loss']\n",
        "        logger.info(f'Eval bpc after pretraining: {eval_loss/math.log(2)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqY_Hg5HbD6a"
      },
      "source": [
        "**Training hyperparameters**\n",
        "\n",
        "- Following RoBERTa pretraining setting, we set number of tokens per batch to be `2^18` tokens. Changing this number might require changes in the lr, lr-scheudler, #steps and #warmup steps. Therefor, it is a good idea to keep this number constant.\n",
        "\n",
        "- Note that: `#tokens/batch = batch_size x #gpus x gradient_accumulation x seqlen`\n",
        "   \n",
        "- In [the paper](https://arxiv.org/pdf/2004.05150.pdf), we train for 65k steps, but 3k is probably enough (check table 6)\n",
        "\n",
        "- **Important note**: The lr-scheduler in [the paper](https://arxiv.org/pdf/2004.05150.pdf) is polynomial_decay with power 3 over 65k steps. To train for 3k steps, use a constant lr-scheduler (after warmup). Both lr-scheduler are not supported in HF trainer, and at least **constant lr-scheduler** will need to be added.\n",
        "\n",
        "- Pretraining will take 2 days on 1 x 32GB GPU with fp32. Consider using fp16 and using more gpus to train faster (if you increase `#gpus`, reduce `gradient_accumulation` to maintain `#tokens/batch` as mentioned earlier).\n",
        "\n",
        "- As a demonstration, this notebook is training on wikitext103 but wikitext103 is rather small that it takes 7 epochs to train for 3k steps Consider doing a single epoch on a larger dataset (800M tokens) instead.\n",
        "\n",
        "- Set #gpus using `CUDA_VISIBLE_DEVICES`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/data/Repos/* /content/data/Repos-cleaned/"
      ],
      "metadata": {
        "id": "--Ud-tJJXmVV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install typescript\n"
      ],
      "metadata": {
        "id": "oPJkyKHgSouO",
        "outputId": "ab4f8207-138d-40aa-f091-c3629ff60c85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0Kâ ‡\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ ‹\u001b[1G\u001b[0K\n",
            "added 1 package in 2s\n",
            "\u001b[1G\u001b[0Kâ ‹\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m\n",
            "\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m New \u001b[31mmajor\u001b[39m version of npm available! \u001b[31m10.8.2\u001b[39m -> \u001b[34m11.0.0\u001b[39m\n",
            "\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m Changelog: \u001b[34mhttps://github.com/npm/cli/releases/tag/v11.0.0\u001b[39m\n",
            "\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m To update run: \u001b[4mnpm install -g npm@11.0.0\u001b[24m\n",
            "\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m\n",
            "\u001b[1G\u001b[0Kâ ‹\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!node /content/CleanRepos.js\n"
      ],
      "metadata": {
        "id": "OzKwye0t05aO",
        "outputId": "81c9b3e7-23ef-4a86-c9ca-300852078f07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config in: /content/data/Repos/1backend/1backend\n",
            "Error processing vs\n",
            "Config in: /content/data/Repos/2fd/graphdoc\n",
            "Config in: /content/data/Repos/43081j/rar.js\n",
            "Config in: /content/data/Repos/500tech/angular-tree-component\n",
            "Config in: /content/data/Repos/5calls/5calls\n",
            "Config in: /content/data/Repos/74th/vscode-vim\n",
            "Config in: /content/data/Repos/AFASSoftware/maquette\n",
            "Config in: /content/data/Repos/Alberplz/angular2-color-picker\n",
            "Skipping: /content/data/Repos/aberezkin/ng2-image-upload/.git\n",
            "Config in: /content/data/Repos/aberezkin/ng2-image-upload/demo\n",
            "Config in: /content/data/Repos/aberezkin/ng2-image-upload/src\n",
            "Config in: /content/data/Repos/accounts-js/accounts\n",
            "Skipping: /content/data/Repos/acekyd/made-in-nigeria/.git\n",
            "Skipping: /content/data/Repos/ademilter/bricklayer/.git\n",
            "Config in: /content/data/Repos/adriancarriger/angularfire2-offline\n",
            "Config in: /content/data/Repos/afrad/angular2-websocket\n",
            "Config in: /content/data/Repos/aggarwalankush/ionic-mosum\n",
            "Config in: /content/data/Repos/aggarwalankush/ionic-push-base\n",
            "Config in: /content/data/Repos/ahomu/Talkie\n",
            "Config in: /content/data/Repos/aikoven/typescript-fsa\n",
            "Config in: /content/data/Repos/aioutecism/amVim-for-VSCode\n",
            "Config in: /content/data/Repos/airbrake/airbrake-js\n",
            "Skipping: /content/data/Repos/ajtowf/ng2_play/.git\n",
            "Config in: /content/data/Repos/ajtowf/ng2_play/e2e\n",
            "Config in: /content/data/Repos/ajtowf/ng2_play/src\n",
            "Config in: /content/data/Repos/akfish/node-vibrant\n",
            "Config in: /content/data/Repos/akserg/ng2-dnd\n",
            "Config in: /content/data/Repos/akserg/ng2-slim-loading-bar\n",
            "Config in: /content/data/Repos/akserg/ng2-toasty\n",
            "Config in: /content/data/Repos/alamgird/angular-next-starter-kit\n",
            "Config in: /content/data/Repos/alefragnani/vscode-project-manager\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!node GetTypes.js\n"
      ],
      "metadata": {
        "id": "1Dd8GS3naxuW",
        "outputId": "4b5f5d80-b2e4-409b-8915-c07eb85840fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config in: data/Repos-cleaned/1backend/1backend\n",
            "Error processing vs\n",
            "Config in: data/Repos-cleaned/2fd/graphdoc\n",
            "Config in: data/Repos-cleaned/43081j/rar.js\n",
            "Config in: data/Repos-cleaned/500tech/angular-tree-component\n",
            "Config in: data/Repos-cleaned/5calls/5calls\n",
            "Config in: data/Repos-cleaned/74th/vscode-vim\n",
            "Config in: data/Repos-cleaned/AFASSoftware/maquette\n",
            "Config in: data/Repos-cleaned/Alberplz/angular2-color-picker\n",
            "Skipping: data/Repos-cleaned/aberezkin/ng2-image-upload/.git\n",
            "Config in: data/Repos-cleaned/aberezkin/ng2-image-upload/demo\n",
            "Config in: data/Repos-cleaned/aberezkin/ng2-image-upload/src\n",
            "Config in: data/Repos-cleaned/accounts-js/accounts\n",
            "!? 1, 0\n",
            "Skipping: data/Repos-cleaned/acekyd/made-in-nigeria/.git\n",
            "Skipping: data/Repos-cleaned/ademilter/bricklayer/.git\n",
            "Config in: data/Repos-cleaned/adriancarriger/angularfire2-offline\n",
            "!? 1, 0\n",
            "Config in: data/Repos-cleaned/afrad/angular2-websocket\n",
            "Config in: data/Repos-cleaned/aggarwalankush/ionic-mosum\n",
            "Config in: data/Repos-cleaned/aggarwalankush/ionic-push-base\n",
            "Config in: data/Repos-cleaned/ahomu/Talkie\n",
            "Config in: data/Repos-cleaned/aikoven/typescript-fsa\n",
            "Config in: data/Repos-cleaned/aioutecism/amVim-for-VSCode\n",
            "Config in: data/Repos-cleaned/airbrake/airbrake-js\n",
            "Skipping: data/Repos-cleaned/ajtowf/ng2_play/.git\n",
            "Config in: data/Repos-cleaned/ajtowf/ng2_play/e2e\n",
            "Config in: data/Repos-cleaned/ajtowf/ng2_play/src\n",
            "Config in: data/Repos-cleaned/akfish/node-vibrant\n",
            "!? 1368, 1364\n",
            "!? 915, 913\n",
            "Config in: data/Repos-cleaned/akserg/ng2-dnd\n",
            "Config in: data/Repos-cleaned/akserg/ng2-slim-loading-bar\n",
            "Config in: data/Repos-cleaned/akserg/ng2-toasty\n",
            "Config in: data/Repos-cleaned/alamgird/angular-next-starter-kit\n",
            "Config in: data/Repos-cleaned/alefragnani/vscode-project-manager\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Zl_hDDlryVo2"
      },
      "outputs": [],
      "source": [
        "\n",
        "@dataclass\n",
        "class ModelArgs:\n",
        "    attention_window: int = field(default=512, metadata={\"help\": \"Size of attention window\"})\n",
        "    max_pos: int = field(default=4096, metadata={\"help\": \"Maximum position\"})\n",
        "\n",
        "parser = HfArgumentParser((TrainingArguments, ModelArgs,))\n",
        "\n",
        "\n",
        "training_args, model_args = parser.parse_args_into_dataclasses(look_for_args_file=False, args=[\n",
        "    '--output_dir', 'tmp',\n",
        "    '--warmup_steps', '500',\n",
        "    '--learning_rate', '0.00003',\n",
        "    '--weight_decay', '0.01',\n",
        "    '--adam_epsilon', '1e-6',\n",
        "    '--max_steps', '3000',\n",
        "    '--logging_steps', '500',\n",
        "    '--save_steps', '500',\n",
        "    '--max_grad_norm', '5.0',\n",
        "    '--per_gpu_eval_batch_size', '8',\n",
        "    '--per_gpu_train_batch_size', '2',  # 32GB gpu with fp32\n",
        "    '--gradient_accumulation_steps', '32',\n",
        "    '--evaluation_strategy','steps',\n",
        "    '--do_train',\n",
        "    '--do_eval',\n",
        "])\n",
        "training_args.val_datapath = '/content/data/outputs-gold'\n",
        "training_args.train_datapath = '/content/data/outputs-all'\n",
        "\n",
        "# Choose GPU\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python lexer.py\n"
      ],
      "metadata": {
        "id": "2YkGFok5mkSo",
        "outputId": "cc80f1af-4d90-4e76-ca23-8b46942de855",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 0: akserg__ng2-toasty.json\n",
            "Processing 1: Alberplz__angular2-color-picker.json\n",
            "Processing 2: airbrake__airbrake-js.json\n",
            "Processing 3: akserg__ng2-slim-loading-bar.json\n",
            "Processing 4: adriancarriger__angularfire2-offline.json\n",
            "Processing 5: 500tech__angular-tree-component.json\n",
            "Processing 6: ahomu__Talkie.json\n",
            "Processing 7: alamgird__angular-next-starter-kit.json\n",
            "Processing 8: 43081j__rar.js.json\n",
            "Processing 9: akserg__ng2-dnd.json\n",
            "Processing 10: 1backend__1backend.json\n",
            "Processing 11: aggarwalankush__ionic-push-base.json\n",
            "Processing 12: afrad__angular2-websocket.json\n",
            "Processing 13: 74th__vscode-vim.json\n",
            "Processing 14: 2fd__graphdoc.json\n",
            "Processing 15: AFASSoftware__maquette.json\n",
            "Processing 16: 5calls__5calls.json\n",
            "Processing 17: akfish__node-vibrant.json\n",
            "Processing 18: aggarwalankush__ionic-mosum.json\n",
            "Processing 19: accounts-js__accounts.json\n",
            "Processing 20: alefragnani__vscode-project-manager.json\n",
            "Processing 21: aioutecism__amVim-for-VSCode.json\n",
            "Processing 22: aikoven__typescript-fsa.json\n",
            "Train projects: 18\n",
            "Validation projects: 2\n",
            "Test projects: 3\n",
            "Train files: 750\n",
            "Validation files: 22\n",
            "Test files: 30\n",
            "Producing vocabularies\n",
            "Size of source vocab: 1397\n",
            "Size of target vocab: 393\n",
            "Writing train/valid/test files\n",
            "Overall tokens: 268789 train, 4487 valid and 15537 test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi5ZIKcsbD6e"
      },
      "source": [
        "### Put it all together\n",
        "\n",
        "1) Evaluating `roberta-base` on MLM to establish a baseline. Validation `bpc` = `2.536` which is higher than the `bpc` values in table 6 [here](https://arxiv.org/pdf/2004.05150.pdf) because wikitext103 is harder than our pretraining corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "JiKj7D1c1ovy",
        "outputId": "ec7ea5c1-2d14-4829-f0db-aee5b0a6708d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'RobertaForMaskedLM' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-48929131c693>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mroberta_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaForMaskedLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'roberta-base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mroberta_base_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaTokenizerFast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'roberta-base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Evaluating roberta-base (seqlen: 512) for refernece ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpretrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroberta_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroberta_base_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RobertaForMaskedLM' is not defined"
          ]
        }
      ],
      "source": [
        "roberta_base = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
        "roberta_base_tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
        "logger.info('Evaluating roberta-base (seqlen: 512) for refernece ...')\n",
        "pretrain_and_evaluate(training_args, roberta_base, roberta_base_tokenizer, eval_only=True, model_path=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBXU3r69bD6l"
      },
      "source": [
        "2) As descriped in `create_long_model`, convert a `roberta-base` model into `roberta-base-4096` which is an instance of `RobertaLong`, then save it to the disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7tcsSfZ1-b9"
      },
      "outputs": [],
      "source": [
        "model_path = f'{training_args.output_dir}/roberta-base-{model_args.max_pos}'\n",
        "if not os.path.exists(model_path):\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "logger.info(f'Converting roberta-base into roberta-base-{model_args.max_pos}')\n",
        "model, tokenizer = create_long_model(\n",
        "    save_model_to=model_path, attention_window=model_args.attention_window, max_pos=model_args.max_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiftMH3-zPUS"
      },
      "source": [
        "3) Load `roberta-base-4096` from the disk. This model works for long sequences even without pretraining. If you don't want to pretrain, you can stop here and start finetuning your `roberta-base-4096` on downstream tasks ðŸŽ‰ðŸŽ‰ðŸŽ‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8vNeYdrzMd2"
      },
      "outputs": [],
      "source": [
        "logger.info(f'Loading the model from {model_path}')\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(model_path)\n",
        "model = RobertaLongForMaskedLM.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS0Np2F4bD6p"
      },
      "source": [
        "4) Pretrain `roberta-base-4096` for `3k` steps, each steps has `2^18` tokens. Notes:\n",
        "\n",
        "- The `training_args.max_steps = 3 ` is just for the demo. **Remove this line for the actual training**\n",
        "\n",
        "- Training for `3k` steps will take 2 days on a single 32GB gpu with `fp32`. Consider using `fp16` and more gpus to train faster.\n",
        "\n",
        "- Tokenizing the training data the first time is going to take 5-10 minutes.\n",
        "\n",
        "- MLM validation `bpc` **before** pretraining: **2.652**, a bit worse than the **2.536** of `roberta-base`. As discussed in [the paper](https://arxiv.org/pdf/2004.05150.pdf) this is expected because the model didn't learn yet to work with the sliding window attention.\n",
        "\n",
        "- MLM validation `bpc` after pretraining for a few number of steps: **2.628**. It is quickly getting better. By 3k steps, it should be better than the **2.536** of `roberta-base`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "f2d298034ac44de397e262fd9902f4cf",
            "dbff0a67283b41c7acc81c9bc7b3c3d2",
            "b8cfa9900fe745f58430dfbd1e86f25e",
            "21cf3ef186aa4895aa9741376834a84e",
            "0dfddf7d39d1402bb83101f580a858b7",
            "d7b45553807e4ec1b6dc691ebcf27a8b",
            "bbed8d4111f548d8a51da4b794c079ac",
            "1fced5a44b4145ebb2b3ea2f40b06b2c"
          ]
        },
        "id": "SHD7QMUWbD6q",
        "outputId": "684d79ed-2237-4c7d-f0b9-dd6ba6b45d00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Pretraining roberta-base-4096 ... \n",
            "INFO:filelock:Lock 140598563391376 acquired on wikitext-103-raw/cached_lm_RobertaTokenizerFast_4094_wiki.valid.raw.lock\n",
            "INFO:transformers.data.datasets.language_modeling:Loading features from cached file wikitext-103-raw/cached_lm_RobertaTokenizerFast_4094_wiki.valid.raw [took 0.017 s]\n",
            "INFO:filelock:Lock 140598563391376 released on wikitext-103-raw/cached_lm_RobertaTokenizerFast_4094_wiki.valid.raw.lock\n",
            "INFO:__main__:Loading and tokenizing training data is usually slow: wikitext-103-raw/wiki.train.raw\n",
            "INFO:filelock:Lock 140599059908048 acquired on wikitext-103-raw/cached_lm_RobertaTokenizerFast_4094_wiki.train.raw.lock\n",
            "INFO:transformers.data.datasets.language_modeling:Loading features from cached file wikitext-103-raw/cached_lm_RobertaTokenizerFast_4094_wiki.train.raw [took 5.838 s]\n",
            "INFO:filelock:Lock 140599059908048 released on wikitext-103-raw/cached_lm_RobertaTokenizerFast_4094_wiki.train.raw.lock\n",
            "INFO:transformers.trainer:You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
            "INFO:transformers.trainer:***** Running Evaluation *****\n",
            "INFO:transformers.trainer:  Num examples = 61\n",
            "INFO:transformers.trainer:  Batch size = 8\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0dfddf7d39d1402bb83101f580a858b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=8.0, style=ProgressStyle(description_widâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Initial eval bpc: 2.6521989344600327\n",
            "INFO:transformers.trainer:***** Running training *****\n",
            "INFO:transformers.trainer:  Num examples = 29114\n",
            "INFO:transformers.trainer:  Num Epochs = 1\n",
            "INFO:transformers.trainer:  Instantaneous batch size per device = 2\n",
            "INFO:transformers.trainer:  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "INFO:transformers.trainer:  Gradient Accumulation steps = 32\n",
            "INFO:transformers.trainer:  Total optimization steps = 3\n",
            "INFO:transformers.trainer:  Starting fine-tuning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "{\"eval_loss\": 1.8383642137050629, \"step\": null}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7b45553807e4ec1b6dc691ebcf27a8b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='iâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbed8d4111f548d8a51da4b794c079ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=14557.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:transformers.trainer:\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "INFO:transformers.trainer:Saving model checkpoint to tmp\n",
            "INFO:transformers.configuration_utils:Configuration saved in tmp/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:transformers.modeling_utils:Model weights saved in tmp/pytorch_model.bin\n",
            "INFO:transformers.trainer:***** Running Evaluation *****\n",
            "INFO:transformers.trainer:  Num examples = 61\n",
            "INFO:transformers.trainer:  Batch size = 8\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1fced5a44b4145ebb2b3ea2f40b06b2c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=8.0, style=ProgressStyle(description_widâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Eval bpc after pretraining: 2.6277886199054827\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "{\"eval_loss\": 1.8214442729949951, \"epoch\": 0.008793020539946418, \"step\": 4}\n"
          ]
        }
      ],
      "source": [
        "logger.info(f'Pretraining roberta-base-{model_args.max_pos} ... ')\n",
        "\n",
        "training_args.max_steps = 3   ## <<<<<<<<<<<<<<<<<<<<<<<< REMOVE THIS <<<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "pretrain_and_evaluate(training_args, model, tokenizer, eval_only=False, model_path=training_args.output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GOc0_6SbD6u"
      },
      "source": [
        "5) Copy global projection layers. MLM pretraining doesn't train global projections, so we need to call `copy_proj_layers` to copy the local projection layers to the global ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obupoA0FbD6v",
        "outputId": "059fa657-a3f4-401d-8230-a9997ffaed67"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Copying local projection layers into global projection layers ... \n",
            "INFO:__main__:Saving model to tmp/roberta-base-4096\n",
            "INFO:transformers.configuration_utils:Configuration saved in tmp/roberta-base-4096/config.json\n",
            "INFO:transformers.modeling_utils:Model weights saved in tmp/roberta-base-4096/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "logger.info(f'Copying local projection layers into global projection layers ... ')\n",
        "model = copy_proj_layers(model)\n",
        "logger.info(f'Saving model to {model_path}')\n",
        "model.save_pretrained(model_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtZ2sfWkbD60"
      },
      "source": [
        "ðŸŽ‰ðŸŽ‰ðŸŽ‰ðŸŽ‰ **DONE**. ðŸŽ‰ðŸŽ‰ðŸŽ‰ðŸŽ‰\n",
        "\n",
        "`model` can now be used for finetuning on downstream tasks after loading it from the disk.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOBmcHTj3NPz"
      },
      "outputs": [],
      "source": [
        "logger.info(f'Loading the model from {model_path}')\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(model_path)\n",
        "model = RobertaLongForMaskedLM.from_pretrained(model_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "convert_model_to_long.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}